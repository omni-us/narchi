

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>narchi.instantiators.pytorch_packed &mdash; narchi 1.3.0 documentation</title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home" alt="Documentation Home"> narchi
          

          
          </a>

          
            
            
              <div class="version">
                1.3.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../schema.html">Json Schema</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cmd.html">Command line tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api.html">API Reference</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">narchi</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>narchi.instantiators.pytorch_packed</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for narchi.instantiators.pytorch_packed</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;A pytorch module instantiator that supports 1d and 2d packed sequences.&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">copy</span> <span class="k">import</span> <span class="n">deepcopy</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="k">import</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">torch.nn.utils.rnn</span> <span class="k">import</span> <span class="n">PackedSequence</span>

<span class="kn">from</span> <span class="nn">.pytorch</span> <span class="k">import</span> <span class="n">BaseModule</span><span class="p">,</span> <span class="n">Reshape</span><span class="p">,</span> <span class="n">standard_pytorch_blocks_mappings</span>


<div class="viewcode-block" id="Packed2dSequence"><a class="viewcode-back" href="../../../api.html#narchi.instantiators.pytorch_packed.Packed2dSequence">[docs]</a><span class="k">class</span> <span class="nc">Packed2dSequence</span><span class="p">(</span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Packed2dSequence&#39;</span><span class="p">,</span> <span class="s1">&#39;data lengths gaps&#39;</span><span class="p">)):</span>
    <span class="sd">&quot;&quot;&quot;Named tuple for packed 2d sequences.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="Packed2dSequence.to"><a class="viewcode-back" href="../../../api.html#narchi.instantiators.pytorch_packed.Packed2dSequence.to">[docs]</a>    <span class="k">def</span> <span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Performs dtype and/or device conversion on `self.data`.&quot;&quot;&quot;</span>
        <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lengths</span><span class="p">,</span> <span class="n">gaps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gaps</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="pack_2d_sequences"><a class="viewcode-back" href="../../../api.html#narchi.instantiators.pytorch_packed.pack_2d_sequences">[docs]</a><span class="k">def</span> <span class="nf">pack_2d_sequences</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">gap_size</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">length_fact</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fail_if_unsorted</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Packs 3-dim tensors into a long 3-dim tensor by concatenating along the last dimension.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (tuple/list[tensor]): Input tensors to pack with shapes [C, H, Wi].</span>
<span class="sd">        gap_size (int): Size for gap between samples.</span>
<span class="sd">        length_fact (int): Increase gaps so that length of inputs are multiple of length_fact.</span>
<span class="sd">        fail_if_unsorted (bool): Whether to raise ValueError if received unsorted input.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Packed2dSequence(data=tensor[1, C, H, lengths+gaps], lenghts=list[len(input)], gaps=[len(input)])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span>
            <span class="ow">and</span> <span class="nb">all</span><span class="p">([</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;shape&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">input</span><span class="p">])</span>
            <span class="ow">and</span> <span class="nb">all</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">input</span><span class="p">])</span>
            <span class="ow">and</span> <span class="nb">all</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="nb">input</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">input</span><span class="p">])</span>
            <span class="ow">and</span> <span class="nb">all</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="nb">input</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">input</span><span class="p">])):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Input required to be a tuple/list of tensors all with 3 dimensions and &#39;</span>
                           <span class="s1">&#39;have the same size for the first two dimensions.&#39;</span><span class="p">)</span>

    <span class="c1">## Check that input is sorted from longest to shortest ##</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">input</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="n">d</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">lengths</span><span class="p">)]):</span>
        <span class="k">if</span> <span class="n">fail_if_unsorted</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Expected input to be sorted from longest to shortest.&#39;</span><span class="p">)</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="nb">input</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">lengths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">input</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

    <span class="c1">## Determine gaps ##</span>
    <span class="n">gaps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="nb">input</span><span class="p">)):</span>
        <span class="n">gaps</span><span class="p">[</span><span class="n">num</span><span class="p">]</span> <span class="o">=</span> <span class="n">gap_size</span> <span class="k">if</span> <span class="n">num</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">fact_off</span> <span class="o">=</span> <span class="p">(</span><span class="n">lengths</span><span class="p">[</span><span class="n">num</span><span class="p">]</span><span class="o">+</span><span class="n">gaps</span><span class="p">[</span><span class="n">num</span><span class="p">])</span> <span class="o">%</span> <span class="n">length_fact</span>
        <span class="k">if</span> <span class="n">fact_off</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">gaps</span><span class="p">[</span><span class="n">num</span><span class="p">]</span> <span class="o">+=</span> <span class="n">length_fact</span> <span class="o">-</span> <span class="n">fact_off</span>

    <span class="c1">## Create tensor for packing ##</span>
    <span class="n">total_length</span> <span class="o">=</span> <span class="n">lengths</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">+</span> <span class="n">gaps</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">packed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="nb">input</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">total_length</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">input</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="nb">input</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># pylint: disable=no-member</span>

    <span class="c1">## Copy input to packed tensor ##</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="nb">input</span><span class="p">)):</span>
        <span class="n">packed</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">offset</span><span class="p">:</span><span class="n">offset</span><span class="o">+</span><span class="n">lengths</span><span class="p">[</span><span class="n">num</span><span class="p">]]</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="n">num</span><span class="p">]</span>
        <span class="n">offset</span> <span class="o">+=</span> <span class="n">lengths</span><span class="p">[</span><span class="n">num</span><span class="p">]</span> <span class="o">+</span> <span class="n">gaps</span><span class="p">[</span><span class="n">num</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">Packed2dSequence</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">packed</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">,</span> <span class="n">gaps</span><span class="o">=</span><span class="n">gaps</span><span class="p">)</span></div>


<div class="viewcode-block" id="packed_2d_set_gaps_to_zero"><a class="viewcode-back" href="../../../api.html#narchi.instantiators.pytorch_packed.packed_2d_set_gaps_to_zero">[docs]</a><span class="k">def</span> <span class="nf">packed_2d_set_gaps_to_zero</span><span class="p">(</span><span class="n">packed</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sets to zero the gap locations of a Packed2dSequence.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">packed</span><span class="p">,</span> <span class="n">Packed2dSequence</span><span class="p">),</span> <span class="s1">&#39;Expected input to be a Packed2dSequence.&#39;</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">packed</span><span class="o">.</span><span class="n">lengths</span>
    <span class="n">gaps</span> <span class="o">=</span> <span class="n">packed</span><span class="o">.</span><span class="n">gaps</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">lengths</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">gaps</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">offset</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">lengths</span><span class="p">)):</span>
            <span class="n">packed</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">offset</span><span class="o">+</span><span class="n">lengths</span><span class="p">[</span><span class="n">num</span><span class="p">]:</span><span class="n">offset</span><span class="o">+</span><span class="n">lengths</span><span class="p">[</span><span class="n">num</span><span class="p">]</span><span class="o">+</span><span class="n">gaps</span><span class="p">[</span><span class="n">num</span><span class="p">]]</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="n">offset</span> <span class="o">+=</span> <span class="n">lengths</span><span class="p">[</span><span class="n">num</span><span class="p">]</span> <span class="o">+</span> <span class="n">gaps</span><span class="p">[</span><span class="n">num</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">packed</span></div>


<div class="viewcode-block" id="packed_2d_to_1d"><a class="viewcode-back" href="../../../api.html#narchi.instantiators.pytorch_packed.packed_2d_to_1d">[docs]</a><span class="k">def</span> <span class="nf">packed_2d_to_1d</span><span class="p">(</span><span class="n">packed_2d</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Converts a Packed2dSequence to a PackedSequence.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">packed_2d</span><span class="p">,</span> <span class="n">Packed2dSequence</span><span class="p">),</span> <span class="s1">&#39;Expected input to be a Packed2dSequence.&#39;</span>

    <span class="n">data</span> <span class="o">=</span> <span class="n">packed_2d</span><span class="o">.</span><span class="n">data</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">packed_2d</span><span class="o">.</span><span class="n">lengths</span>
    <span class="n">gaps</span> <span class="o">=</span> <span class="n">packed_2d</span><span class="o">.</span><span class="n">gaps</span>

    <span class="c1">## Reserve memory for packed 1d tensor ##</span>
    <span class="n">packed_1d_length</span> <span class="o">=</span> <span class="n">lengths</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">packed_1d_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">packed_1d_length</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">]))</span>
    <span class="n">data_1d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">packed_1d_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># pylint: disable=no-member</span>

    <span class="c1">## Get packed 2d sequence offsets and batch_sizes ##</span>
    <span class="n">offsets</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">batch_sizes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">lengths</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>  <span class="c1"># pylint: disable=no-member</span>
    <span class="k">for</span> <span class="n">num</span><span class="p">,</span> <span class="n">length</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lengths</span><span class="p">):</span>
        <span class="n">offsets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">offsets</span><span class="p">[</span><span class="n">num</span><span class="p">]</span><span class="o">+</span><span class="n">length</span><span class="o">+</span><span class="n">gaps</span><span class="p">[</span><span class="n">num</span><span class="p">])</span>
        <span class="n">batch_sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">length</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1">## Copy data to packed 1d tensor ##</span>
    <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">p</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">nsamp</span> <span class="ow">in</span> <span class="n">batch_sizes</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nsamp</span><span class="p">):</span>
            <span class="n">data_1d</span><span class="p">[</span><span class="n">p</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">offsets</span><span class="p">[</span><span class="n">num</span><span class="p">]</span><span class="o">+</span><span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">p</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1">## Return PackedSequence ##</span>
    <span class="n">sorted_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">num</span> <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">lengths</span><span class="p">))],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># pylint: disable=not-callable,no-member</span>
    <span class="k">return</span> <span class="n">PackedSequence</span><span class="p">(</span><span class="n">data_1d</span><span class="p">,</span> <span class="n">batch_sizes</span><span class="p">,</span> <span class="n">sorted_indices</span><span class="p">,</span> <span class="n">sorted_indices</span><span class="p">)</span></div>


<div class="viewcode-block" id="ReshapePacked"><a class="viewcode-back" href="../../../api.html#narchi.instantiators.pytorch_packed.ReshapePacked">[docs]</a><span class="k">class</span> <span class="nc">ReshapePacked</span><span class="p">(</span><span class="n">Reshape</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Reshape module that works with Packed2dSequence.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="ReshapePacked.forward"><a class="viewcode-back" href="../../../api.html#narchi.instantiators.pytorch_packed.ReshapePacked.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Transforms the shape of the input according to the specification in reshape_spec.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Packed2dSequence</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape_spec</span> <span class="o">!=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;Reshape of Packed2dSequence only supported for reshape_spec=[2, [0, 1]].&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">packed_2d_to_1d</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="Conv2dPacked"><a class="viewcode-back" href="../../../api.html#narchi.instantiators.pytorch_packed.Conv2dPacked">[docs]</a><span class="k">class</span> <span class="nc">Conv2dPacked</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Extension of torch.nn.Conv2d that works with Packed2dSequence.</span>

<span class="sd">    After convolving, the gaps are set to zero to guaranty that these values are</span>
<span class="sd">    not considered when computing gradients or subsequent forwards.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="Conv2dPacked.__init__"><a class="viewcode-back" href="../../../api.html#narchi.instantiators.pytorch_packed.Conv2dPacked.__init__">[docs]</a>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">kernel_size</span><span class="o">//</span><span class="mi">2</span> <span class="o">!=</span> <span class="n">padding</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;Currently only implemented for kernel_size//2 == padding.&#39;</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="n">out_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="c1">#stride=stride,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
            <span class="c1">#dilation=dilation,</span>
            <span class="c1">#groups=groups,</span>
            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
            <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_gap</span> <span class="o">=</span> <span class="n">kernel_size</span><span class="o">//</span><span class="mi">2</span></div>

<div class="viewcode-block" id="Conv2dPacked.forward"><a class="viewcode-back" href="../../../api.html#narchi.instantiators.pytorch_packed.Conv2dPacked.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Packed2dSequence</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">g</span><span class="o">&lt;</span><span class="bp">self</span><span class="o">.</span><span class="n">min_gap</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="nb">input</span><span class="o">.</span><span class="n">gaps</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;Gaps too small to prevent interference between samples, min_gap=</span><span class="si">{self.min_gap}</span><span class="s1"> &#39;</span>
                               <span class="n">f</span><span class="s1">&#39;gaps={list(input.gaps[:-1])}.&#39;</span><span class="p">)</span>
        <span class="n">output_data</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="c1">#_save_image_detached(output_data[0,0:3,:,:], &#39;post_conv.png&#39;)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">packed_2d_set_gaps_to_zero</span><span class="p">(</span><span class="n">Packed2dSequence</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">output_data</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">lengths</span><span class="p">,</span> <span class="n">gaps</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">gaps</span><span class="p">))</span>
        <span class="c1">#_save_image_detached(output.data[0,0:3,:,:], &#39;post_conv_mask.png&#39;)</span>
        <span class="k">return</span> <span class="n">output</span></div></div>


<div class="viewcode-block" id="MaxPool2dPacked"><a class="viewcode-back" href="../../../api.html#narchi.instantiators.pytorch_packed.MaxPool2dPacked">[docs]</a><span class="k">class</span> <span class="nc">MaxPool2dPacked</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Extension of torch.nn.MaxPool2d that works with Packed2dSequence.</span>

<span class="sd">    The lengths and gaps are re-estimated.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="MaxPool2dPacked.__init__"><a class="viewcode-back" href="../../../api.html#narchi.instantiators.pytorch_packed.MaxPool2dPacked.__init__">[docs]</a>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">kernel_size</span> <span class="o">!=</span> <span class="n">stride</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;Currently only implemented for kernel_size == stride.&#39;</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">)</span></div>

<div class="viewcode-block" id="MaxPool2dPacked.forward"><a class="viewcode-back" href="../../../api.html#narchi.instantiators.pytorch_packed.MaxPool2dPacked.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Packed2dSequence</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

        <span class="c1">## Compute new packed lengths and gaps ##</span>
        <span class="n">lengths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">lengths</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">gaps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">gaps</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">lengths</span><span class="p">)):</span>
            <span class="n">length_in</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">lengths</span><span class="p">[</span><span class="n">num</span><span class="p">]</span>
            <span class="n">gap_in</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">gaps</span><span class="p">[</span><span class="n">num</span><span class="p">]</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">length_in</span><span class="o">+</span><span class="n">gap_in</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;Expected sum of lengths of sample and gap be a multiple of pooling kernel &#39;</span>
                                   <span class="n">f</span><span class="s1">&#39;size: num=</span><span class="si">{num}</span><span class="s1"> kernel_size=</span><span class="si">{self.kernel_size}</span><span class="s1"> length=</span><span class="si">{length_in}</span><span class="s1"> gap=</span><span class="si">{gap_in}</span><span class="s1">.&#39;</span><span class="p">)</span>
            <span class="c1">#if self.ceil_mode:</span>
            <span class="n">lengths</span><span class="p">[</span><span class="n">num</span><span class="p">]</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">length_in</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>
            <span class="c1">#else:</span>
            <span class="c1">#    lengths[num] = math.floor(length_in/self.kernel_size)</span>
            <span class="n">gaps</span><span class="p">[</span><span class="n">num</span><span class="p">]</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">gap_in</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>

        <span class="c1">## Perform pooling ##</span>
        <span class="c1">#_save_image_detached(input.data[0,0:3,:,:], &#39;pre_maxpool.png&#39;)</span>
        <span class="n">output_data</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="c1">#_save_image_detached(output_data[0,0:3,:,:], &#39;post_maxpool.png&#39;)</span>

        <span class="c1">## Check that pooled tensor matches shape ##</span>
        <span class="k">if</span> <span class="n">lengths</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">+</span><span class="n">gaps</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">!=</span> <span class="n">output_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Bug in implementation, pooling length differs w.r.t. lengths and gaps computation.&#39;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">Packed2dSequence</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">output_data</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">,</span> <span class="n">gaps</span><span class="o">=</span><span class="n">gaps</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="BatchNorm2dPacked"><a class="viewcode-back" href="../../../api.html#narchi.instantiators.pytorch_packed.BatchNorm2dPacked">[docs]</a><span class="k">class</span> <span class="nc">BatchNorm2dPacked</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Extension of torch.nn.BatchNorm2d that works with Packed2dSequence.</span>

<span class="sd">    After normalization, the gaps are set to zero to guaranty that these values</span>
<span class="sd">    are not considered when computing gradients or subsequent forwards.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="BatchNorm2dPacked.forward"><a class="viewcode-back" href="../../../api.html#narchi.instantiators.pytorch_packed.BatchNorm2dPacked.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Packed2dSequence</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">output_data</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="c1">#_save_image_detached(output_data[0,0:3,:,:], &#39;post_bnorm.png&#39;)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">packed_2d_set_gaps_to_zero</span><span class="p">(</span><span class="n">Packed2dSequence</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">output_data</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">lengths</span><span class="p">,</span> <span class="n">gaps</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">gaps</span><span class="p">))</span>
        <span class="c1">#_save_image_detached(output.data[0,0:3,:,:], &#39;post_bnorm_mask.png&#39;)</span>
        <span class="k">return</span> <span class="n">output</span></div></div>


<div class="viewcode-block" id="LeakyReLU2dPacked"><a class="viewcode-back" href="../../../api.html#narchi.instantiators.pytorch_packed.LeakyReLU2dPacked">[docs]</a><span class="k">class</span> <span class="nc">LeakyReLU2dPacked</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Extension of torch.nn.LeakyReLU that works with Packed2dSequence.&quot;&quot;&quot;</span>
<div class="viewcode-block" id="LeakyReLU2dPacked.forward"><a class="viewcode-back" href="../../../api.html#narchi.instantiators.pytorch_packed.LeakyReLU2dPacked.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Packed2dSequence</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">output_data</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Packed2dSequence</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">output_data</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">lengths</span><span class="p">,</span> <span class="n">gaps</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">gaps</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="Linear1dPacked"><a class="viewcode-back" href="../../../api.html#narchi.instantiators.pytorch_packed.Linear1dPacked">[docs]</a><span class="k">class</span> <span class="nc">Linear1dPacked</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Extension of torch.nn.Linear that works with PackedSequence.&quot;&quot;&quot;</span>
<div class="viewcode-block" id="Linear1dPacked.forward"><a class="viewcode-back" href="../../../api.html#narchi.instantiators.pytorch_packed.Linear1dPacked.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">PackedSequence</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">output_data</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">PackedSequence</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">output_data</span><span class="p">,</span> <span class="n">batch_sizes</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">batch_sizes</span><span class="p">,</span> <span class="n">sorted_indices</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">sorted_indices</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="LogSoftmaxPacked"><a class="viewcode-back" href="../../../api.html#narchi.instantiators.pytorch_packed.LogSoftmaxPacked">[docs]</a><span class="k">class</span> <span class="nc">LogSoftmaxPacked</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Extension of torch.nn.LogSoftmax that works with PackedSequence.&quot;&quot;&quot;</span>
<div class="viewcode-block" id="LogSoftmaxPacked.forward"><a class="viewcode-back" href="../../../api.html#narchi.instantiators.pytorch_packed.LogSoftmaxPacked.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">PackedSequence</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">output_data</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">PackedSequence</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">output_data</span><span class="p">,</span> <span class="n">batch_sizes</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">batch_sizes</span><span class="p">,</span> <span class="n">sorted_indices</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">sorted_indices</span><span class="p">)</span></div></div>


<span class="n">mappings</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;Reshape&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;class&#39;</span><span class="p">:</span> <span class="s1">&#39;narchi.instantiators.pytorch_packed.ReshapePacked&#39;</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s1">&#39;Conv2d&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;class&#39;</span><span class="p">:</span> <span class="s1">&#39;narchi.instantiators.pytorch_packed.Conv2dPacked&#39;</span><span class="p">,</span>
        <span class="s1">&#39;kwargs&#39;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s1">&#39;in_channels&#39;</span><span class="p">:</span> <span class="s1">&#39;shape:in:0&#39;</span><span class="p">,</span>
            <span class="s1">&#39;out_channels&#39;</span><span class="p">:</span> <span class="s1">&#39;output_feats&#39;</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">},</span>
    <span class="s1">&#39;MaxPool2d&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;class&#39;</span><span class="p">:</span> <span class="s1">&#39;narchi.instantiators.pytorch_packed.MaxPool2dPacked&#39;</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s1">&#39;BatchNorm2d&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;class&#39;</span><span class="p">:</span> <span class="s1">&#39;narchi.instantiators.pytorch_packed.BatchNorm2dPacked&#39;</span><span class="p">,</span>
        <span class="s1">&#39;kwargs&#39;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s1">&#39;num_features&#39;</span><span class="p">:</span> <span class="s1">&#39;shape:in:0&#39;</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">},</span>
    <span class="s1">&#39;LogSoftmax&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;class&#39;</span><span class="p">:</span> <span class="s1">&#39;narchi.instantiators.pytorch_packed.LogSoftmaxPacked&#39;</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s1">&#39;LeakyReLU&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;class&#39;</span><span class="p">:</span> <span class="s1">&#39;narchi.instantiators.pytorch_packed.LeakyReLU2dPacked&#39;</span><span class="p">,</span>
        <span class="s1">&#39;kwargs&#39;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s1">&#39;inplace&#39;</span><span class="p">:</span> <span class="s1">&#39;const:bool:True&#39;</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">},</span>
    <span class="s1">&#39;Linear&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;class&#39;</span><span class="p">:</span> <span class="s1">&#39;narchi.instantiators.pytorch_packed.Linear1dPacked&#39;</span><span class="p">,</span>
        <span class="s1">&#39;kwargs&#39;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s1">&#39;in_features&#39;</span><span class="p">:</span> <span class="s1">&#39;shape:in:-1&#39;</span><span class="p">,</span>
            <span class="s1">&#39;out_features&#39;</span><span class="p">:</span> <span class="s1">&#39;output_feats&#39;</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">},</span>
<span class="p">}</span>

<span class="n">packed_pytorch_blocks_mappings</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">standard_pytorch_blocks_mappings</span><span class="p">)</span>
<span class="n">packed_pytorch_blocks_mappings</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">mappings</span><span class="p">)</span>


<div class="viewcode-block" id="PackedModule"><a class="viewcode-back" href="../../../api.html#narchi.instantiators.pytorch_packed.PackedModule">[docs]</a><span class="k">class</span> <span class="nc">PackedModule</span><span class="p">(</span><span class="n">BaseModule</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Class for instantiating pytorch modules that support 1d and 2d packed sequences.&quot;&quot;&quot;</span>

    <span class="n">blocks_mappings</span> <span class="o">=</span> <span class="n">packed_pytorch_blocks_mappings</span>

<div class="viewcode-block" id="PackedModule.__init__"><a class="viewcode-back" href="../../../api.html#narchi.instantiators.pytorch_packed.PackedModule.__init__">[docs]</a>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">gap_size</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">length_fact</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initializer for PackedModule class.</span>

<span class="sd">        Args:</span>
<span class="sd">            gap_size (int): Size for gap between samples.</span>
<span class="sd">            length_fact (int): Increase gaps so that length of inputs are multiple of length_fact.</span>
<span class="sd">            args/kwargs: All other arguments accepted by :class:`.BaseModule`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">BaseModule</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gap_size</span> <span class="o">=</span> <span class="n">gap_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">length_fact</span> <span class="o">=</span> <span class="n">length_fact</span></div>


<div class="viewcode-block" id="PackedModule.inputs_preprocess"><a class="viewcode-back" href="../../../api.html#narchi.instantiators.pytorch_packed.PackedModule.inputs_preprocess">[docs]</a>    <span class="k">def</span> <span class="nf">inputs_preprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">values</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Converts tuples of tensors into Packed2dSequence.</span>

<span class="sd">        Args:</span>
<span class="sd">            values (OrderedDict): Inputs to the module.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">values</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">Packed2dSequence</span><span class="p">):</span>
                <span class="n">values</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">pack_2d_sequences</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">gap_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gap_size</span><span class="p">,</span> <span class="n">length_fact</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">length_fact</span><span class="p">)</span></div></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2018-present, Mauricio Villegas

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>